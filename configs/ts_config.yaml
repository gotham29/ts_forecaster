train_models: False
do_gridsearch: False
scale: minmax  #minmax, standard, robust, False
test_prop: 0.3
data_cap: 100
forecast_horizon: 1
time_col: timestamp
eval_metric: rmse #mae, mse, rmse, mape, mase, ope, marre, r2_score, dtw_metric

features:
  in:
    - x_
    - y_
    - z_
  pred:
    - x_
    - y_
    - z_

modnames_grids:
  LightGBMModel:
    lags:
      # - 1
      - 5
      - 10
    # lags_past_covariates: Union[int, List[int]] = None,
    # lags_future_covariates: Union[Tuple[int, int], List[int]] = None,
    # output_chunk_length: int = 1,
    # quantiles: List[float] = None,
    # random_state: Optional[int] = None,
    # add_encoders: Optional[dict] = None,
      # = {
      # 'cyclic': {'future': ['month']},
      # 'datetime_attribute': {'future': ['hour', 'dayofweek']},
      # 'position': {'past': ['absolute'], 'future': ['relative']},
      # 'custom': {'past': [lambda idx: (idx.year - 1950) / 50]},
      # 'transformer': Scaler()
      # }
    # likelihood: str = None,
    #   = ['quantile', 'poisson']
  VARIMA:
    p:
      - 3
      - 6
    d:
      - 0
    q:
      - 3
    # trend: 'c' ['', '', '']
  NBEATSModel:
    # output_chunk_length: int,
    input_chunk_length:
      - 5
      - 10
    num_stacks:
      - 10
      # - 30
    num_blocks:
      - 1
      # - 2
    num_layers:
      - 1
      # - 2
    layer_widths:
      - 128
      # - 256
    dropout:
      # - 0.0
      - 0.2
    # generic_architecture: True
    # expansion_coefficient_dim: 5
    # trend_polynomial_degree: 2
    # activation: "ReLU"  ## ['ReLU','RReLU', 'PReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU',  'Sigmoid']
  TCNModel:
    # output_chunk_length: int,
    input_chunk_length:
      - 5
      - 10
    num_filters:
      - 3
    kernel_size:
      - 3
    dilation_base:
      - 2
    weight_norm:
      - False
    dropout:
      # - 0.2
      - 0.4
    # num_layers: int,
    # dropout_fn,
  TransformerModel:
    # output_chunk_length: int,
    input_chunk_length:
      - 5
      - 10
    d_model:
      - 64
    nhead:
      - 2
      # - 4
    num_encoder_layers:
      - 2
      # - 3
    num_decoder_layers:
      - 2
      # - 3
    dim_feedforward:
      - 512
    dropout:
      - 0.1
      # - 0.2
    activation:
      - "relu"  ## ["relu", "gelu"]
    # custom_encoder: Optional[nn.Module] = None,
    # custom_decoder: Optional[nn.Module] = None,
  RNNModel:
    # input_chunk_length: int,
    input_chunk_length:
      - 5
      - 10
    training_length:
      # - 5
      - 10
    model: ## ["RNN", "LSTM", "GRU"]
      # - RNN
      - LSTM
    hidden_dim:
      # - 10
      - 25
    n_rnn_layers:
      - 1
      # - 2
    dropout:
      # - 0.0
      - 0.2

