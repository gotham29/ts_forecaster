test_prop: 0.3
data_cap: 300
train_models: True
forecast_horizon: 1

time_col: timestamp
loss_metric: mape

dirs:
  data_in: /Users/samheiserman/Desktop/repos/ts_forecaster/data/sample_timeseries_nab.csv
  data_out: /Users/samheiserman/Desktop/repos/ts_forecaster/output/data
  models_out: /Users/samheiserman/Desktop/repos/ts_forecaster/output/models
  results_out: /Users/samheiserman/Desktop/repos/ts_forecaster/output/results

features:
  in:
    - x_
    - y_
    - z_
  pred:
    - x_
    - y_
    - z_

modnames_grids:
  # VARIMA:
  #   p:
  #     - 1
  #     - 3
  #   d:
  #     - 0
  #   q:
  #     # - 1
  #     - 3
  #   # trend: 'c' ['', '', '']
  # NBEATSModel:
  #   # output_chunk_length: int,
  #   input_chunk_length:
  #     - 5
  #     # - 10
  #   num_stacks:
  #     - 10
  #     # - 30
  #   num_blocks:
  #     - 1
  #     # - 2
  #   num_layers:
  #     - 1
  #     # - 2
  #   layer_widths:
  #     - 128
  #     # - 256
  #   dropout:
  #     - 0.0
  #     - 0.2
  #   # generic_architecture: True
  #   # expansion_coefficient_dim: 5
  #   # trend_polynomial_degree: 2
  #   # activation: "ReLU"  ## ['ReLU','RReLU', 'PReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU',  'Sigmoid']
  # TCNModel:
  #   # output_chunk_length: int,
  #   input_chunk_length:
  #     - 5
  #     - 10
  #   num_filters:
  #     - 3
  #   kernel_size:
  #     - 3
  #   dilation_base:
  #     - 2
  #   weight_norm:
  #     - False
  #   dropout:
  #     # - 0.2
  #     - 0.4
  #   # num_layers: int,
  #   # dropout_fn,
  # TransformerModel:
  #   # output_chunk_length: int,
  #   input_chunk_length:
  #     - 5
  #     - 10
  #   d_model:
  #     - 64
  #   nhead:
  #     - 2
  #     # - 4
  #   num_encoder_layers:
  #     - 2
  #     # - 3
  #   num_decoder_layers:
  #     - 2
  #     # - 3
  #   dim_feedforward:
  #     - 512
  #   dropout:
  #     - 0.1
  #     # - 0.2
  #   activation:
  #     - "relu"  ## ["relu", "gelu"]
  #   # custom_encoder: Optional[nn.Module] = None,
  #   # custom_decoder: Optional[nn.Module] = None,
  # RNNModel:
  #   # input_chunk_length: int,
  #   input_chunk_length:
  #     - 5
  #     # - 10
  #   training_length:
  #     - 5
  #     # - 10
  #   model: ## ["RNN", "LSTM", "GRU"]
  #     # - RNN
  #     - LSTM
  #   hidden_dim:
  #     # - 10
  #     - 25
  #   n_rnn_layers:
  #     - 1
  #     - 2
  #   dropout:
  #     # - 0.0
  #     - 0.2
  #   # training_length: int = 24,
  LightGBMModel:
    lags:
      # - 1
      - 5
      - 10
    # lags_past_covariates: Union[int, List[int]] = None,
    # lags_future_covariates: Union[Tuple[int, int], List[int]] = None,
    # output_chunk_length: int = 1,
    # quantiles: List[float] = None,
    # random_state: Optional[int] = None,
    # add_encoders: Optional[dict] = None,
      # = {
      # 'cyclic': {'future': ['month']},
      # 'datetime_attribute': {'future': ['hour', 'dayofweek']},
      # 'position': {'past': ['absolute'], 'future': ['relative']},
      # 'custom': {'past': [lambda idx: (idx.year - 1950) / 50]},
      # 'transformer': Scaler()
      # }
    # likelihood: str = None,
    #   = ['quantile', 'poisson']
